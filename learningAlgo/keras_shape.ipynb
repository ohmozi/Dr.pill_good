{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기부터 CNN모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shy9546/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 16)        256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 16)        48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        8192      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        96        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 64)          192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 75,858\n",
      "Trainable params: 75,634\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "Found 770 images belonging to 2 classes.\n",
      "Found 160 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "# Initialising the CNN\n",
    "# classifier = Sequential()\n",
    "model = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "# classifier.add(Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "# classifier.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "# classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# classifier.add(Dropout(0.25))\n",
    "\n",
    "# classifier.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "# classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# classifier.add(Dropout(0.25))\n",
    "\n",
    "# classifier.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "# classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# classifier.add(Dropout(0.25))\n",
    "\n",
    "# classifier.add(Flatten())\n",
    "# classifier.add(Dense(512, activation='relu'))\n",
    "# classifier.add(Dropout(0.5))\n",
    "# classifier.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.add(Conv2D(16, (4, 4), padding='same', use_bias=False, input_shape=(64, 64, 1)))   # 사진 사이즈 조정\n",
    "model.add(BatchNormalization(axis=3, scale=False))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\n",
    "model.add(Dropout(0.2))        # 앙상블 효과 \n",
    "\n",
    "model.add(Conv2D(32, (4, 4), padding='same', use_bias=False))\n",
    "model.add(BatchNormalization(axis=3, scale=False))\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Flatten())\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (4, 4), padding='same', use_bias=False))\n",
    "model.add(BatchNormalization(axis=3, scale=False))\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Flatten())\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(Conv2D(128, (3, 3), padding='same', use_bias=False))\n",
    "#model.add(BatchNormalization(axis=3, scale=False))\n",
    "#model.add(Activation(\"relu\"))\n",
    "model.add(Flatten())\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(2, activation='softmax'))     # 판단할 카테고리 수  9\n",
    "model.summary()\n",
    "\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)   #스케일링을 줄여주는것 (normalization 적용해보면 더 좋아질 수 있음) \n",
    "                                   \n",
    "                                   #validation_split=0.33)\n",
    "                                   #otation_range=10,\n",
    "                                   #width_shift_range=0.2,\n",
    "                                   #height_shift_range=0.2,\n",
    "                                   #shear_range=0.2)\n",
    "                                   #zoom_range=[0.9, 2.2],\n",
    "                                   #horizontal_flip=True,\n",
    "                                   #vertical_flip=True,\n",
    "                                   #fill_mode='nearest')\n",
    "                                   #validation_split=0.33)\n",
    "\n",
    "#test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/home/shy9546/Downloads/last_check/train',   # 훈련시킬 사진 경로\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=13,\n",
    "                                                 target_size = (64, 64),    # 사이즈 지정해주기\n",
    "                                                 batch_size = 15,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 #color_mode='rgb',\n",
    "                                                 class_mode = 'categorical')\n",
    "                                                 #subset=\"training\")\n",
    "validation_set = train_datagen.flow_from_directory('/home/shy9546/Downloads/last_check/test',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=13,\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 10,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 #color_mode='rgb',\n",
    "                                                 class_mode = 'categorical')\n",
    "                                                 #subset=\"validation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "52/52 [==============================] - 8s 150ms/step - loss: 0.6920 - acc: 0.6038 - val_loss: 0.4786 - val_acc: 0.7562\n",
      "Epoch 2/30\n",
      "52/52 [==============================] - 7s 127ms/step - loss: 0.3392 - acc: 0.8501 - val_loss: 0.1277 - val_acc: 0.9562\n",
      "Epoch 3/30\n",
      "52/52 [==============================] - 7s 127ms/step - loss: 0.1672 - acc: 0.9320 - val_loss: 0.0356 - val_acc: 0.9875\n",
      "Epoch 4/30\n",
      "52/52 [==============================] - 7s 126ms/step - loss: 0.1135 - acc: 0.9577 - val_loss: 0.1980 - val_acc: 0.9125\n",
      "Epoch 5/30\n",
      "52/52 [==============================] - 6s 123ms/step - loss: 0.1015 - acc: 0.9655 - val_loss: 1.0973 - val_acc: 0.5875\n",
      "Epoch 6/30\n",
      "52/52 [==============================] - 6s 121ms/step - loss: 0.0753 - acc: 0.9718 - val_loss: 0.0347 - val_acc: 0.9937\n",
      "Epoch 7/30\n",
      "52/52 [==============================] - 7s 126ms/step - loss: 0.0461 - acc: 0.9859 - val_loss: 0.0169 - val_acc: 0.9937\n",
      "Epoch 8/30\n",
      "52/52 [==============================] - 6s 124ms/step - loss: 0.0355 - acc: 0.9910 - val_loss: 0.0221 - val_acc: 0.9937\n",
      "Epoch 9/30\n",
      "52/52 [==============================] - 6s 122ms/step - loss: 0.0612 - acc: 0.9795 - val_loss: 0.4135 - val_acc: 0.8625\n",
      "Epoch 10/30\n",
      "52/52 [==============================] - 6s 122ms/step - loss: 0.0523 - acc: 0.9795 - val_loss: 0.1381 - val_acc: 0.9500\n",
      "Epoch 11/30\n",
      "52/52 [==============================] - 6s 120ms/step - loss: 0.0144 - acc: 0.9962 - val_loss: 0.0106 - val_acc: 0.9937\n",
      "Epoch 12/30\n",
      "52/52 [==============================] - 6s 125ms/step - loss: 0.0231 - acc: 0.9910 - val_loss: 0.0346 - val_acc: 0.9875\n",
      "Epoch 13/30\n",
      "52/52 [==============================] - 6s 124ms/step - loss: 0.0399 - acc: 0.9872 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "52/52 [==============================] - 7s 126ms/step - loss: 0.0358 - acc: 0.9910 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "52/52 [==============================] - 6s 122ms/step - loss: 0.0343 - acc: 0.9859 - val_loss: 0.1334 - val_acc: 0.9500\n",
      "Epoch 16/30\n",
      "52/52 [==============================] - 6s 122ms/step - loss: 0.0307 - acc: 0.9897 - val_loss: 0.0124 - val_acc: 0.9937\n",
      "Epoch 17/30\n",
      "52/52 [==============================] - 6s 123ms/step - loss: 0.0255 - acc: 0.9885 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 0.0255 - acc: 0.9885 - val_loss: 0.0109 - val_acc: 0.9937\n",
      "Epoch 19/30\n",
      "52/52 [==============================] - 6s 119ms/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.1053 - val_acc: 0.9625\n",
      "Epoch 20/30\n",
      "52/52 [==============================] - 6s 120ms/step - loss: 0.0120 - acc: 0.9949 - val_loss: 0.0240 - val_acc: 0.9937\n",
      "Epoch 21/30\n",
      "52/52 [==============================] - 7s 133ms/step - loss: 0.0217 - acc: 0.9936 - val_loss: 0.2074 - val_acc: 0.9125\n",
      "Epoch 22/30\n",
      "52/52 [==============================] - 7s 128ms/step - loss: 0.0270 - acc: 0.9898 - val_loss: 0.0097 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "52/52 [==============================] - 5s 105ms/step - loss: 0.0201 - acc: 0.9962 - val_loss: 0.0064 - val_acc: 0.9937\n",
      "Epoch 24/30\n",
      "52/52 [==============================] - 5s 93ms/step - loss: 0.0105 - acc: 0.9974 - val_loss: 1.3124 - val_acc: 0.6000\n",
      "Epoch 25/30\n",
      "52/52 [==============================] - 5s 95ms/step - loss: 0.0238 - acc: 0.9962 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "52/52 [==============================] - 5s 100ms/step - loss: 0.0106 - acc: 0.9974 - val_loss: 0.0225 - val_acc: 0.9937\n",
      "Epoch 27/30\n",
      "52/52 [==============================] - 5s 95ms/step - loss: 0.0056 - acc: 0.9974 - val_loss: 0.0246 - val_acc: 0.9937\n",
      "Epoch 28/30\n",
      "52/52 [==============================] - 5s 95ms/step - loss: 0.0103 - acc: 0.9974 - val_loss: 0.0735 - val_acc: 0.9812\n",
      "Epoch 29/30\n",
      "52/52 [==============================] - 5s 97ms/step - loss: 0.0070 - acc: 0.9962 - val_loss: 0.0528 - val_acc: 0.9875\n",
      "Epoch 30/30\n",
      "52/52 [==============================] - 5s 95ms/step - loss: 0.0096 - acc: 0.9962 - val_loss: 0.0340 - val_acc: 0.9937\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('./log.csv', append=True, separator=';')\n",
    "\n",
    "##hist = model.fit_generator(training_set,\n",
    "#                         epochs = 50,   # 반복수 변경가능\n",
    "#                         validation_data = validation_set\n",
    "#                         )\n",
    "STEP_SIZE_TRAIN = training_set.n\n",
    "STEP_SIZE_VALID = validation_set.n\n",
    "model.fit_generator(generator = training_set,\n",
    "                    #steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                   validation_data=validation_set,\n",
    "                    #validation_steps=STEP_SIZE_VALID,\n",
    "                   epochs=30)\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('cnn_attraction_keras_shape.h5')\n",
    "\n",
    "# output = classifier.predict_generator(test_set, steps=5)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "prediction_set = train_datagen.flow_from_directory('/home/shy9546/Downloads/color_temp/test',\n",
    "                                                  target_size=(64,64),\n",
    "                                                  color_mode='grayscale',\n",
    "                                                  #color_mode='rgb',\n",
    "                                                  batch_size=1,\n",
    "                                                  class_mode =None,\n",
    "                                                  shuffle=False,\n",
    "                                                  seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluate --\n",
      "acc: 99.37%\n",
      "-- Predict --\n",
      "10/10 [==============================] - 0s 25ms/step\n",
      "['elipse', 'elipse', 'elipse', 'elipse', 'elipse', 'circle', 'circle', 'elipse', 'elipse', 'elipse']\n",
      "[[1.3740821e-10 1.0000000e+00]\n",
      " [1.2161780e-07 9.9999988e-01]\n",
      " [2.0282746e-06 9.9999797e-01]\n",
      " [4.2040793e-09 1.0000000e+00]\n",
      " [1.1627523e-10 1.0000000e+00]\n",
      " [9.3885255e-01 6.1147496e-02]\n",
      " [9.7461390e-01 2.5386054e-02]\n",
      " [5.7554310e-03 9.9424452e-01]\n",
      " [5.9771584e-03 9.9402279e-01]\n",
      " [3.3686626e-01 6.6313380e-01]]\n",
      "['test_set/10.png', 'test_set/11.png', 'test_set/12.png', 'test_set/13.png', 'test_set/14.png', 'test_set/15.png', 'test_set/2.png', 'test_set/3.png', 'test_set/4.png', 'test_set/8.png']\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "#model.load('./cnn_attraction_keras_model11.h5')\n",
    "#model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model = load_model('./cnn_attraction_keras_shape.h5')\n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    "\n",
    "scores = model.evaluate_generator(\n",
    "            validation_set)\n",
    "            #steps = 10)\n",
    "\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    "STEP_SIZE_TEST = prediction_set.n\n",
    "prediction_set.reset()\n",
    "\n",
    "output = model.predict_generator(\n",
    "            prediction_set,\n",
    "            verbose=1)\n",
    "\n",
    "predicted_class_indices = np.argmax(output,axis=1)\n",
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "print(predictions)\n",
    "\n",
    "print(output)\n",
    "print(prediction_set.filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model10 -> circle picture error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
